{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib2\n",
    "import urllib\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_obj(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "def merge_d(x, y):\n",
    "    z = x.copy()\n",
    "    z.update(y)\n",
    "    return z\n",
    "\n",
    "def flatten_data(t_data):\n",
    "    t_data_f= {}\n",
    "    for key in t_data.keys():\n",
    "        for key2 in t_data[key].keys():\n",
    "            t_data_f[key + '_' + key2] = t_data[key][key2]\n",
    "    return t_data_f\n",
    "\n",
    "twit_data = flatten_data(load_obj('twit_data1'))\n",
    "t_data_num = 1\n",
    "\n",
    "def backup(obj, t_data_num = t_data_num):\n",
    "    name = \"t_df\" + str(t_data_num)\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to do some data wrangling to aggregate all the data from the Tweets. We want to get information about the tweet text, the number of likes, the number of retweets, names, and href. Later on, we can extract more analytics about these types of interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent progress 0.0 running for 5.08666038513e-05 mins\n",
      "Percent progress 5.0 running for 0.929983933767 mins\n",
      "Percent progress 10.0 running for 2.19964901606 mins\n",
      "Percent progress 15.0 running for 3.53876201709 mins\n",
      "Percent progress 20.0 running for 4.88092761834 mins\n",
      "Percent progress 25.0 running for 6.11316250165 mins\n",
      "Percent progress 30.0 running for 7.68281498353 mins\n",
      "Percent progress 35.0 running for 9.68449503183 mins\n",
      "Percent progress 40.0 running for 11.6382258654 mins\n",
      "Percent progress 45.0 running for 13.2810905496 mins\n",
      "Percent progress 50.0 running for 15.5552463651 mins\n",
      "Percent progress 55.0 running for 17.8359643817 mins\n",
      "Percent progress 60.0 running for 20.021214668 mins\n",
      "Percent progress 65.0 running for 22.598205018 mins\n",
      "Percent progress 70.0 running for 25.0689711491 mins\n",
      "Percent progress 75.0 running for 28.2786712845 mins\n",
      "Percent progress 80.0 running for 31.1675331672 mins\n",
      "Percent progress 85.0 running for 34.0141220649 mins\n",
      "Percent progress 90.0 running for 37.2621017337 mins\n",
      "Percent progress 95.0 running for 40.6350143154 mins\n"
     ]
    }
   ],
   "source": [
    "company_l = []\n",
    "series_l = []\n",
    "text_l = [] \n",
    "time_l = []\n",
    "retweets_l = []\n",
    "likes_l = []\n",
    "name_l = []\n",
    "href_l = []\n",
    "errors = []\n",
    "max_ind = len(twit_data.keys())\n",
    "start = time.time()\n",
    "\n",
    "for i in range(max_ind):    \n",
    "    \n",
    "    # Timing code and backup code\n",
    "    if i % 50 == 0:\n",
    "        mins = (time.time() - start) / 60\n",
    "        print 'Percent progress ' + str(100.0 * i / max_ind) + ' running for ' + str(mins) + ' mins'          \n",
    "        data_backup = {'Company': company_l, 'Series': series_l, 'Text': text_l, 'Time':time_l, 'Retweets':retweets_l, 'Likes':likes_l, 'Name':name_l, 'Href':href_l}\n",
    "        backup(data_backup)\n",
    "    \n",
    "    # Finding the series of the data\n",
    "    company_series = twit_data.keys()[i]\n",
    "    ind = company_series[:company_series.rfind(\"_\")].rfind(\"_\")\n",
    "    company = company_series[:ind]\n",
    "    series = company_series[ind:]\n",
    "    backup({'Company': company_l, 'Series': series_l, 'Text': text_l, 'Time':time_l, 'Retweets':retweets_l, 'Likes':likes_l, 'Name':name_l, 'Href':href_l})\n",
    "    \n",
    "    \n",
    "    # Loop through each copmany and series pair and extract relevant information\n",
    "    for tweet in twit_data[company_series]:\n",
    "        try:\n",
    "            # Searching\n",
    "            soup = BeautifulSoup(tweet, 'lxml')\n",
    "            actions = soup.find_all('div', class_='ProfileTweet-actionCountList')[0]\n",
    "            text = soup.find_all('p', class_ = 'TweetTextSize')[0].text\n",
    "            times = soup.find_all('a', class_ = 'tweet-timestamp')[0].text\n",
    "            retweets = actions.find_all('span', class_='ProfileTweet-actionCount')[0].text.strip()\n",
    "            likes = actions.find_all('span', class_='ProfileTweet-actionCount')[1].text.strip()\n",
    "            name = soup.find_all('strong', class_='fullname')[0].text\n",
    "            href = soup.find_all('a', class_='account-group')[0]['href']\n",
    "            \n",
    "            # Storing\n",
    "            company_l.append(company)\n",
    "            series_l.append(series)\n",
    "            text_l.append(text)\n",
    "            time_l.append(times)\n",
    "            retweets_l.append(retweets)\n",
    "            likes_l.append(likes)\n",
    "            name_l.append(name)\n",
    "            href_l.append(href)\n",
    "            \n",
    "        # Only listen for keyboard exceptions\n",
    "        except KeyboardInterrupt:\n",
    "            try:\n",
    "                sys.exit(0)\n",
    "            except SystemExit:\n",
    "                os._exit(0) \n",
    "        except:\n",
    "            errors.append(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After extracting all tweets from the HTML page, we now want to try to generate features that we can regress on. We save the tweets to a file, and then extract features from all of these tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/candokevin/anaconda/lib/python2.7/site-packages/pandas/io/parsers.py:1170: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = self._reader.read(nrows)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Company</th>\n",
       "      <th>Href</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Name</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Series</th>\n",
       "      <th>Text</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Occipital</td>\n",
       "      <td>/yehyehgluglu</td>\n",
       "      <td>0 likes</td>\n",
       "      <td>Maria Melo</td>\n",
       "      <td>0 retweets</td>\n",
       "      <td>_Series_A</td>\n",
       "      <td>@rubaums2 mira in the occipital and frontal to...</td>\n",
       "      <td>14 Jul 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Occipital</td>\n",
       "      <td>/ArturoMisifu</td>\n",
       "      <td>0 likes</td>\n",
       "      <td>Arturo Quijano</td>\n",
       "      <td>0 retweets</td>\n",
       "      <td>_Series_A</td>\n",
       "      <td>at the height of the occipital head - hurts if...</td>\n",
       "      <td>14 Jul 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>Occipital</td>\n",
       "      <td>/victorcab</td>\n",
       "      <td>0 likes</td>\n",
       "      <td>Victor Caballero</td>\n",
       "      <td>0 retweets</td>\n",
       "      <td>_Series_A</td>\n",
       "      <td>Tripod – Timer for 360 Views – iPhone? : http:...</td>\n",
       "      <td>14 Jul 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>Occipital</td>\n",
       "      <td>/electroniko</td>\n",
       "      <td>0 likes</td>\n",
       "      <td>MIGUEL NEGRETE</td>\n",
       "      <td>0 retweets</td>\n",
       "      <td>_Series_A</td>\n",
       "      <td>Neurona, glia, axon myelin, parietal, temporal...</td>\n",
       "      <td>14 Jul 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>Occipital</td>\n",
       "      <td>/nomina_bot</td>\n",
       "      <td>0 likes</td>\n",
       "      <td>体野ミナ</td>\n",
       "      <td>0 retweets</td>\n",
       "      <td>_Series_A</td>\n",
       "      <td>Greater occipital nerve:nervus occipitalis maj...</td>\n",
       "      <td>13 Jul 2011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 Unnamed: 0.1    Company           Href    Likes              Name    Retweets     Series                                               Text         Time\n",
       "0           0            0  Occipital  /yehyehgluglu  0 likes        Maria Melo  0 retweets  _Series_A  @rubaums2 mira in the occipital and frontal to...  14 Jul 2011\n",
       "1           1            2  Occipital  /ArturoMisifu  0 likes    Arturo Quijano  0 retweets  _Series_A  at the height of the occipital head - hurts if...  14 Jul 2011\n",
       "2           2            5  Occipital     /victorcab  0 likes  Victor Caballero  0 retweets  _Series_A  Tripod – Timer for 360 Views – iPhone? : http:...  14 Jul 2011\n",
       "3           3            7  Occipital   /electroniko  0 likes    MIGUEL NEGRETE  0 retweets  _Series_A  Neurona, glia, axon myelin, parietal, temporal...  14 Jul 2011\n",
       "4           4            8  Occipital    /nomina_bot  0 likes              体野ミナ  0 retweets  _Series_A  Greater occipital nerve:nervus occipitalis maj...  13 Jul 2011"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.read_csv(open(\"../tweets/tweets.csv\", 'rU'), encoding='utf-8', engine='c')\n",
    "tweets_translated = pd.read_csv(open(\"../startups/translatedTweets.csv\", 'rU'), encoding='utf-8', engine='c')\n",
    "tweets_translated.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only take tweets that relate to companies that have 150 or more tweets. We also want to calculate the average number of days it took to reach 200 tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculating relative dates based off of dates like Apr, 5 2015. We can use this later\n",
    "# for calculating average number of tweets per day.\n",
    "months = {'Jan':1, 'Feb':2, 'Mar':3, 'Apr':4, 'May':5, 'Jun':6, 'Jul':7, 'Aug':8, 'Sep':9, 'Oct':10, 'Nov':11, 'Dec':12}\n",
    "time_sub = []\n",
    "for row in tweets.iterrows():\n",
    "    if type(row[1]['Time']) != unicode:\n",
    "        time_sub.append(0)\n",
    "    elif len(row[1]['Time'].split(' ')) == 3:\n",
    "        if row[1]['Time'].split(' ')[1] in months.keys():        \n",
    "            time_sub.append(int(row[1]['Time'].split(' ')[2]) * 365 + months[row[1]['Time'].split(' ')[1]] * 30 + int(row[1]['Time'].split(' ')[0]))\n",
    "        else:\n",
    "            time_sub.append(0)\n",
    "    else:\n",
    "        time_sub.append(0)\n",
    "        \n",
    "tweets['Date_Num'] = time_sub\n",
    "tweets = tweets[tweets['Date_Num'] != 0]\n",
    "\n",
    "valid_data = []\n",
    "for val in zip(list((tweets.groupby(['Company', 'Series']).size() > 150).index), list((tweets.groupby(['Company', 'Series']).size() > 150).values)):\n",
    "    if val[1]:\n",
    "        valid_data.append(val[0])\n",
    "        \n",
    "# We also only filter the tweets based off of whether or not there was 150 tweets in the past\n",
    "valid_tweets = []\n",
    "for valid_pair in valid_data:\n",
    "    company = valid_pair[0]\n",
    "    series = valid_pair[1]\n",
    "    match_company = tweets[tweets['Company'] == company]\n",
    "    valid_tweets.append(match_company[match_company['Series'] == series])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We are now incorporate the translated tweets that Roger generated\n",
    "valid_tweets_pd = pd.concat(valid_tweets)\n",
    "for row in tweets_translated.iterrows():\n",
    "    company = row[1]['Company']\n",
    "    href = row[1]['Href']\n",
    "    time = row[1]['Time']\n",
    "\n",
    "    match_comp = valid_tweets_pd[valid_tweets_pd['Company'] == company]\n",
    "    match_href = match_comp[match_comp['Href'] == href]\n",
    "    match_time = match_href[match_href['Time'] == time]\n",
    "    \n",
    "    res = list(match_time.index)\n",
    "    if len(res) == 1:\n",
    "        index = res[0]\n",
    "        valid_tweets_pd.set_value(index, 'Text', row[1].Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "valid_tweets_pd.to_csv('valid_tweets.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198201"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets_translated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the total number of tweets is now 2/3 of what it used to be. It seems reasonable to drop this many tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_tweets_pd = pd.concat(valid_tweets)\n",
    "valid_tweets_pd['Likes'] = valid_tweets_pd.apply(lambda row: int(row['Likes'][0]), axis=1)\n",
    "valid_tweets_pd['Retweets'] = valid_tweets_pd.apply(lambda row: int(row['Retweets'][0]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to extract relevant features like the number of likes, retweets, and other relevant features for each company funding round pairing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate average tweets\n",
    "days = valid_tweets_pd.groupby(['Company', 'Series'])['Date_Num'].max() - valid_tweets_pd.groupby(['Company', 'Series'])['Date_Num'].min()\n",
    "tweets = valid_tweets_pd.groupby(['Company', 'Series']).size()\n",
    "avg_tweets = tweets / days\n",
    "\n",
    "# Calculate total likes\n",
    "total_likes= valid_tweets_pd.groupby(['Company', 'Series'])['Likes'].sum()\n",
    "\n",
    "# Calculate total retweets\n",
    "total_retweets = valid_tweets_pd.groupby(['Company', 'Series'])['Retweets'].sum()\n",
    "companies = [val[0] for val in list(tweets.index)]\n",
    "series = [val[1] for val in list(tweets.index)]\n",
    "\n",
    "# Calculate all features\n",
    "tweet_features = pd.DataFrame({'Index': range(1, 1+len(days)), 'Company':companies, 'Series':series, 'Total_Likes': total_likes, 'Total_Retweets': total_retweets, 'Avg_Tweets': avg_tweets})\n",
    "tweet_features.index = range(1, 1+len(days))\n",
    "\n",
    "# Open old funding data\n",
    "funding_df=pd.read_csv(\"funding_collapsed.csv\")\n",
    "funding_df = funding_df.drop(['Unnamed: 0'], 1)\n",
    "funding_df.rename(columns={'Names':'Company', 'Series_Type':'Series'}, inplace=True)\n",
    "\n",
    "# Merge the tweet features with the funding data\n",
    "regress_features = pd.merge(tweet_features, funding_df, how='left', on=['Company', 'Series'])\n",
    "regress_features = regress_features.drop('Index', 1)\n",
    "regress_features = regress_features[([type(amount) == str for amount in list(regress_features.Series_Amount)])]\n",
    "regress_features['Series_Amount'] = regress_features.apply(lambda row: int(row['Series_Amount'][1:].replace(',','')), axis=1)\n",
    "regress_features.to_csv('regress_features.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
