{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib2\n",
    "import urllib\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_obj(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "def merge_d(x, y):\n",
    "    z = x.copy()\n",
    "    z.update(y)\n",
    "    return z\n",
    "\n",
    "def flatten_data(t_data):\n",
    "    t_data_f= {}\n",
    "    for key in t_data.keys():\n",
    "        for key2 in t_data[key].keys():\n",
    "            t_data_f[key + '_' + key2] = t_data[key][key2]\n",
    "    return t_data_f\n",
    "\n",
    "twit_data = flatten_data(load_obj('twit_data1'))\n",
    "t_data_num = 1\n",
    "\n",
    "def backup(obj, t_data_num = t_data_num):\n",
    "    name = \"t_df\" + str(t_data_num)\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent progress 0.0 running for 5.08666038513e-05 mins\n",
      "Percent progress 5.0 running for 0.929983933767 mins\n",
      "Percent progress 10.0 running for 2.19964901606 mins\n",
      "Percent progress 15.0 running for 3.53876201709 mins\n",
      "Percent progress 20.0 running for 4.88092761834 mins\n",
      "Percent progress 25.0 running for 6.11316250165 mins\n",
      "Percent progress 30.0 running for 7.68281498353 mins\n",
      "Percent progress 35.0 running for 9.68449503183 mins\n",
      "Percent progress 40.0 running for 11.6382258654 mins\n",
      "Percent progress 45.0 running for 13.2810905496 mins\n",
      "Percent progress 50.0 running for 15.5552463651 mins\n",
      "Percent progress 55.0 running for 17.8359643817 mins\n",
      "Percent progress 60.0 running for 20.021214668 mins\n",
      "Percent progress 65.0 running for 22.598205018 mins\n",
      "Percent progress 70.0 running for 25.0689711491 mins\n",
      "Percent progress 75.0 running for 28.2786712845 mins\n",
      "Percent progress 80.0 running for 31.1675331672 mins\n",
      "Percent progress 85.0 running for 34.0141220649 mins\n",
      "Percent progress 90.0 running for 37.2621017337 mins\n",
      "Percent progress 95.0 running for 40.6350143154 mins\n"
     ]
    }
   ],
   "source": [
    "company_l = []\n",
    "series_l = []\n",
    "text_l = [] \n",
    "time_l = []\n",
    "retweets_l = []\n",
    "likes_l = []\n",
    "name_l = []\n",
    "href_l = []\n",
    "errors = []\n",
    "max_ind = len(twit_data.keys())\n",
    "start = time.time()\n",
    "\n",
    "for i in range(max_ind):    \n",
    "    if i % 50 == 0:\n",
    "        mins = (time.time() - start) / 60\n",
    "        print 'Percent progress ' + str(100.0 * i / max_ind) + ' running for ' + str(mins) + ' mins'          \n",
    "        data_backup = {'Company': company_l, 'Series': series_l, 'Text': text_l, 'Time':time_l, 'Retweets':retweets_l, 'Likes':likes_l, 'Name':name_l, 'Href':href_l}\n",
    "        backup(data_backup)\n",
    "    company_series = twit_data.keys()[i]\n",
    "    ind = company_series[:company_series.rfind(\"_\")].rfind(\"_\")\n",
    "    company = company_series[:ind]\n",
    "    series = company_series[ind:]\n",
    "    backup({'Company': company_l, 'Series': series_l, 'Text': text_l, 'Time':time_l, 'Retweets':retweets_l, 'Likes':likes_l, 'Name':name_l, 'Href':href_l})\n",
    "    for tweet in twit_data[company_series]:\n",
    "        try:\n",
    "            # Searching\n",
    "            soup = BeautifulSoup(tweet, 'lxml')\n",
    "            actions = soup.find_all('div', class_='ProfileTweet-actionCountList')[0]\n",
    "            text = soup.find_all('p', class_ = 'TweetTextSize')[0].text\n",
    "            times = soup.find_all('a', class_ = 'tweet-timestamp')[0].text\n",
    "            retweets = actions.find_all('span', class_='ProfileTweet-actionCount')[0].text.strip()\n",
    "            likes = actions.find_all('span', class_='ProfileTweet-actionCount')[1].text.strip()\n",
    "            name = soup.find_all('strong', class_='fullname')[0].text\n",
    "            href = soup.find_all('a', class_='account-group')[0]['href']\n",
    "            \n",
    "            # Storing\n",
    "            company_l.append(company)\n",
    "            series_l.append(series)\n",
    "            text_l.append(text)\n",
    "            time_l.append(times)\n",
    "            retweets_l.append(retweets)\n",
    "            likes_l.append(likes)\n",
    "            name_l.append(name)\n",
    "            href_l.append(href)\n",
    "        except KeyboardInterrupt:\n",
    "            try:\n",
    "                sys.exit(0)\n",
    "            except SystemExit:\n",
    "                os._exit(0) \n",
    "        except:\n",
    "            errors.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(open(\"../tweets/tweets.csv\", 'rU'), encoding='utf-8', engine='c')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7144123594703834"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.uniform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96842\n"
     ]
    }
   ],
   "source": [
    "START = 4\n",
    "END = 8\n",
    "WIN_PROB = 0.7\n",
    "TOTAL_GAMES_PLAYED = 100000\n",
    "\n",
    "games_played = 0\n",
    "games_won = 0\n",
    "\n",
    "for i in xrange(TOTAL_GAMES_PLAYED):\n",
    "    now = START\n",
    "    while now < END and now > 0:\n",
    "        rand = np.random.uniform()\n",
    "        if rand < WIN_PROB:\n",
    "            now += 1\n",
    "        else:\n",
    "            now -= 1\n",
    "    games_played += 1\n",
    "    if now == END:\n",
    "        games_won += 1\n",
    "\n",
    "print 1.*games_won/games_played\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
=======
   "execution_count": 336,
>>>>>>> 2d96cae1a734f42c3ac9b8a1873c4ca54e969ad5
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "months = {'Jan':1, 'Feb':2, 'Mar':3, 'Apr':4, 'May':5, 'Jun':6, 'Jul':7, 'Aug':8, 'Sep':9, 'Oct':10, 'Nov':11, 'Dec':12}\n",
    "time_sub = []\n",
    "for row in tweets.iterrows():\n",
    "    if type(row[1]['Time']) != unicode:\n",
    "        time_sub.append(0)\n",
    "    elif len(row[1]['Time'].split(' ')) == 3:\n",
    "        if row[1]['Time'].split(' ')[1] in months.keys():        \n",
    "            time_sub.append(int(row[1]['Time'].split(' ')[2]) * 365 + months[row[1]['Time'].split(' ')[1]] * 30 + int(row[1]['Time'].split(' ')[0]))\n",
    "        else:\n",
    "            time_sub.append(0)\n",
    "    else:\n",
    "        time_sub.append(0)\n",
    "        \n",
    "tweets['Date_Num'] = time_sub\n",
    "tweets = tweets[tweets['Date_Num'] != 0]\n",
    "\n",
    "valid_data = []\n",
    "for val in zip(list((tweets.groupby(['Company', 'Series']).size() > 50).index), list((tweets.groupby(['Company', 'Series']).size() > 150).values)):\n",
    "    if val[1]:\n",
    "        valid_data.append(val[0])\n",
    "        \n",
    "valid_tweets = []\n",
    "for valid_pair in valid_data:\n",
    "    company = valid_pair[0]\n",
    "    series = valid_pair[1]\n",
    "    match_company = tweets[tweets['Company'] == company]\n",
    "    valid_tweets.append(match_company[match_company['Series'] == series])\n",
    "    \n",
    "valid_tweets_pd = pd.concat(valid_tweets)\n",
    "valid_tweets_pd['Likes'] = valid_tweets_pd.apply(lambda row: int(row['Likes'][0]), axis=1)\n",
    "valid_tweets_pd['Retweets'] = valid_tweets_pd.apply(lambda row: int(row['Retweets'][0]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "days = valid_tweets_pd.groupby(['Company', 'Series'])['Date_Num'].max() - valid_tweets_pd.groupby(['Company', 'Series'])['Date_Num'].min()\n",
    "tweets = valid_tweets_pd.groupby(['Company', 'Series']).size()\n",
    "avg_tweets = tweets / days\n",
    "total_likes= valid_tweets_pd.groupby(['Company', 'Series'])['Likes'].sum()\n",
    "total_retweets = valid_tweets_pd.groupby(['Company', 'Series'])['Retweets'].sum()\n",
    "companies = [val[0] for val in list(tweets.index)]\n",
    "series = [val[1] for val in list(tweets.index)]\n",
    "tweet_features = pd.DataFrame({'Index': range(1, 1+len(days)), 'Company':companies, 'Series':series, 'Total_Likes': total_likes, 'Total_Retweets': total_retweets, 'Avg_Tweets': avg_tweets})\n",
    "tweet_features.index = range(1, 1+len(days))\n",
    "funding_df=pd.read_csv(\"funding_collapsed.csv\")\n",
    "funding_df = funding_df.drop(['Unnamed: 0'], 1)\n",
    "funding_df.rename(columns={'Names':'Company', 'Series_Type':'Series'}, inplace=True)\n",
    "funding_df.head()\n",
    "regress_features = pd.merge(tweet_features, funding_df, how='left', on=['Company', 'Series'])\n",
    "regress_features = regress_features.drop('Index', 1)\n",
    "regress_features = regress_features[([type(amount) == str for amount in list(regress_features.Series_Amount)])]\n",
    "regress_features['Series_Amount'] = regress_features.apply(lambda row: int(row['Series_Amount'][1:].replace(',','')), axis=1)\n",
    "regress_features.head()\n",
    "regress_features.to_csv('regress_features.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
