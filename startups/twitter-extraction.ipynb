{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib2\n",
    "import urllib\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_obj(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "def merge_d(x, y):\n",
    "    z = x.copy()\n",
    "    z.update(y)\n",
    "    return z\n",
    "\n",
    "def flatten_data(t_data):\n",
    "    t_data_f= {}\n",
    "    for key in t_data.keys():\n",
    "        for key2 in t_data[key].keys():\n",
    "            t_data_f[key + '_' + key2] = t_data[key][key2]\n",
    "    return t_data_f\n",
    "\n",
    "twit_data = flatten_data(load_obj('twit_data1'))\n",
    "t_data_num = 1\n",
    "\n",
    "def backup(obj, t_data_num = t_data_num):\n",
    "    name = \"t_df\" + str(t_data_num)\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent progress 0.0 running for 5.08666038513e-05 mins\n",
      "Percent progress 5.0 running for 0.929983933767 mins\n",
      "Percent progress 10.0 running for 2.19964901606 mins\n",
      "Percent progress 15.0 running for 3.53876201709 mins\n",
      "Percent progress 20.0 running for 4.88092761834 mins\n",
      "Percent progress 25.0 running for 6.11316250165 mins\n",
      "Percent progress 30.0 running for 7.68281498353 mins\n",
      "Percent progress 35.0 running for 9.68449503183 mins\n",
      "Percent progress 40.0 running for 11.6382258654 mins\n",
      "Percent progress 45.0 running for 13.2810905496 mins\n",
      "Percent progress 50.0 running for 15.5552463651 mins\n",
      "Percent progress 55.0 running for 17.8359643817 mins\n",
      "Percent progress 60.0 running for 20.021214668 mins\n",
      "Percent progress 65.0 running for 22.598205018 mins\n",
      "Percent progress 70.0 running for 25.0689711491 mins\n",
      "Percent progress 75.0 running for 28.2786712845 mins\n",
      "Percent progress 80.0 running for 31.1675331672 mins\n",
      "Percent progress 85.0 running for 34.0141220649 mins\n",
      "Percent progress 90.0 running for 37.2621017337 mins\n",
      "Percent progress 95.0 running for 40.6350143154 mins\n"
     ]
    }
   ],
   "source": [
    "company_l = []\n",
    "series_l = []\n",
    "text_l = [] \n",
    "time_l = []\n",
    "retweets_l = []\n",
    "likes_l = []\n",
    "name_l = []\n",
    "href_l = []\n",
    "errors = []\n",
    "max_ind = len(twit_data.keys())\n",
    "start = time.time()\n",
    "\n",
    "for i in range(max_ind):    \n",
    "    if i % 50 == 0:\n",
    "        mins = (time.time() - start) / 60\n",
    "        print 'Percent progress ' + str(100.0 * i / max_ind) + ' running for ' + str(mins) + ' mins'          \n",
    "        data_backup = {'Company': company_l, 'Series': series_l, 'Text': text_l, 'Time':time_l, 'Retweets':retweets_l, 'Likes':likes_l, 'Name':name_l, 'Href':href_l}\n",
    "        backup(data_backup)\n",
    "    company_series = twit_data.keys()[i]\n",
    "    ind = company_series[:company_series.rfind(\"_\")].rfind(\"_\")\n",
    "    company = company_series[:ind]\n",
    "    series = company_series[ind:]\n",
    "    backup({'Company': company_l, 'Series': series_l, 'Text': text_l, 'Time':time_l, 'Retweets':retweets_l, 'Likes':likes_l, 'Name':name_l, 'Href':href_l})\n",
    "    for tweet in twit_data[company_series]:\n",
    "        try:\n",
    "            # Searching\n",
    "            soup = BeautifulSoup(tweet, 'lxml')\n",
    "            actions = soup.find_all('div', class_='ProfileTweet-actionCountList')[0]\n",
    "            text = soup.find_all('p', class_ = 'TweetTextSize')[0].text\n",
    "            times = soup.find_all('a', class_ = 'tweet-timestamp')[0].text\n",
    "            retweets = actions.find_all('span', class_='ProfileTweet-actionCount')[0].text.strip()\n",
    "            likes = actions.find_all('span', class_='ProfileTweet-actionCount')[1].text.strip()\n",
    "            name = soup.find_all('strong', class_='fullname')[0].text\n",
    "            href = soup.find_all('a', class_='account-group')[0]['href']\n",
    "            \n",
    "            # Storing\n",
    "            company_l.append(company)\n",
    "            series_l.append(series)\n",
    "            text_l.append(text)\n",
    "            time_l.append(times)\n",
    "            retweets_l.append(retweets)\n",
    "            likes_l.append(likes)\n",
    "            name_l.append(name)\n",
    "            href_l.append(href)\n",
    "        except KeyboardInterrupt:\n",
    "            try:\n",
    "                sys.exit(0)\n",
    "            except SystemExit:\n",
    "                os._exit(0) \n",
    "        except:\n",
    "            errors.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets = pd.DataFrame({'Company': company_l, 'Series': series_l, 'Text': text_l, 'Time':time_l, 'Retweets':retweets_l, 'Likes':likes_l, 'Name':name_l, 'Href':href_l})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'tweets1.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-af39c3bfc76c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtweets_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tweets1.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rU'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtweets_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tweets2.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rU'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtweets_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tweets3.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rU'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtweets_4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tweets4.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rU'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtweets_5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tweets5.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rU'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'tweets1.csv'"
     ]
    }
   ],
   "source": [
    "tweets_1 = pd.read_csv(open(\"tweets1.csv\", 'rU'), encoding='utf-8', engine='c')\n",
    "tweets_2 = pd.read_csv(open(\"tweets2.csv\", 'rU'), encoding='utf-8', engine='c')\n",
    "tweets_3 = pd.read_csv(open(\"tweets3.csv\", 'rU'), encoding='utf-8', engine='c')\n",
    "tweets_4 = pd.read_csv(open(\"tweets4.csv\", 'rU'), encoding='utf-8', engine='c')\n",
    "tweets_5 = pd.read_csv(open(\"tweets5.csv\", 'rU'), encoding='utf-8', engine='c')\n",
    "tweets_6 = pd.read_csv(open(\"tweets6.csv\", 'rU'), encoding='utf-8', engine='c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>Href</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Name</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Series</th>\n",
       "      <th>Text</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SiliconBlueTechnologies</td>\n",
       "      <td>/twlatestnews</td>\n",
       "      <td>0 likes</td>\n",
       "      <td>台灣新聞</td>\n",
       "      <td>0 retweets</td>\n",
       "      <td>_Series_D</td>\n",
       "      <td>SiliconBlue出貨增(中央商情網): （中央社台北2010年10月12日電）美國商業...</td>\n",
       "      <td>12 Oct 2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36Kr</td>\n",
       "      <td>/idksanna</td>\n",
       "      <td>1 like</td>\n",
       "      <td>sanna</td>\n",
       "      <td>0 retweets</td>\n",
       "      <td>_Series_C</td>\n",
       "      <td>@idksanna I ordered them I have 0,36kr left :-)</td>\n",
       "      <td>Jan 14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36Kr</td>\n",
       "      <td>/RetailAnalytica</td>\n",
       "      <td>0 likes</td>\n",
       "      <td>Retail Analytica</td>\n",
       "      <td>0 retweets</td>\n",
       "      <td>_Series_C</td>\n",
       "      <td>微卤老板娘晒O2O经营数据：我是如何3个月业绩破200万的 - 36kr http://ia...</td>\n",
       "      <td>Jan 13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36Kr</td>\n",
       "      <td>/just4v2ex</td>\n",
       "      <td>0 likes</td>\n",
       "      <td>just4v2ex</td>\n",
       "      <td>0 retweets</td>\n",
       "      <td>_Series_C</td>\n",
       "      <td>[酷工作] 做优美的产品，做伟大的公司：寻找中国优秀的设计师、程序员加入:\\n\\n列举几个事...</td>\n",
       "      <td>Jan 11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36Kr</td>\n",
       "      <td>/TheNewsChina2</td>\n",
       "      <td>0 likes</td>\n",
       "      <td>TheNewsChina</td>\n",
       "      <td>0 retweets</td>\n",
       "      <td>_Series_C</td>\n",
       "      <td>36kr 8点1氪：中国超过美国成为最大的iPhone消费市场 36kr 虽然欧洲标准的电动...</td>\n",
       "      <td>Jan 11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Company              Href    Likes              Name    Retweets     Series                                               Text         Time\n",
       "0  SiliconBlueTechnologies     /twlatestnews  0 likes              台灣新聞  0 retweets  _Series_D  SiliconBlue出貨增(中央商情網): （中央社台北2010年10月12日電）美國商業...  12 Oct 2010\n",
       "1                     36Kr         /idksanna   1 like             sanna  0 retweets  _Series_C    @idksanna I ordered them I have 0,36kr left :-)       Jan 14\n",
       "2                     36Kr  /RetailAnalytica  0 likes  Retail Analytica  0 retweets  _Series_C  微卤老板娘晒O2O经营数据：我是如何3个月业绩破200万的 - 36kr http://ia...       Jan 13\n",
       "3                     36Kr        /just4v2ex  0 likes         just4v2ex  0 retweets  _Series_C  [酷工作] 做优美的产品，做伟大的公司：寻找中国优秀的设计师、程序员加入:\\n\\n列举几个事...       Jan 11\n",
       "4                     36Kr    /TheNewsChina2  0 likes      TheNewsChina  0 retweets  _Series_C  36kr 8点1氪：中国超过美国成为最大的iPhone消费市场 36kr 虽然欧洲标准的电动...       Jan 11"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merge"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
