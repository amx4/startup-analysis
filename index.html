<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="Startup-analysis : ">

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <title>Predicting Startup Funding Via Twitter</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/kyang01/startup-analysis">View on GitHub</a>

          <h1 id="project_title">Predicting Startup Funding Via Twitter</h1>
          <h4 class="project_tagline">JKMR Data</h4>
          <h4 class="project_tagline">Roger Zou, Melody Guan, Kevin Yang, Jerry Anunrojwong</h4>
          <a class='shortcut' href="https://kyang01.github.io/startup-analysis#video">Screencast</a>
          &emsp; &emsp; &emsp; &emsp;<a class='shortcut' href="https://kyang01.github.io/startup-analysis#overview">Overview</a>
          &emsp; &emsp; &emsp; &emsp;<a class='shortcut' href="https://kyang01.github.io/startup-analysis#analysis">Analysis</a>
          &emsp; &emsp; &emsp; &emsp;<a class='shortcut' href="https://kyang01.github.io/startup-analysis#predictions">Predictions</a>
            <section id="downloads">
              <a class="zip_download_link" href="https://github.com/kyang01/startup-analysis/zipball/master">Download this project as a .zip file</a>
              <a class="tar_download_link" href="https://github.com/kyang01/startup-analysis/tarball/master">Download this project as a tar.gz file</a>
            </section>
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
      <div id="video">
<iframe width="560" height="315" src="https://www.youtube.com/embed/8DSeZji2x-Y" frameborder="0" allowfullscreen></iframe>
</div>
      
      <h3>
<a id="welcome-to-github-pages" class="anchor" href="#welcome-to-github-pages" aria-hidden="true"><span class="octicon octicon-link"></span></a>Welcome to GitHub Pages.</h3>

<p>The web site should effectively summarize the main results of your project and tell a story. Consider your audience (the site is public) and keep the level of discussion at the appropriate level. Your iPython process book and data should be linked to the web site as well, either using a zip file, github, bitbucket, or another code hosting site. Also embed your main visualizations and your screencast in your website.</p>
      <h2>
<a id="overview" class="anchor" href="#" aria-hidden="true"><span class="octicon octicon-link"></span></a>Overview</h2>

<h3>Motivation and Goals</h3>

<!--a id="overview" class="anchor" href="#overview" aria-hidden="true"><span class="octicon octicon-link"></span></a>Overview</h3>
Overview</h3>
-->
<p>For the average public relations officer, manipulating the media is a standby. Social media is no exception, with sites like Twitter, Instagram, and Facebook now regular tools in a publicist's arsenal. Raising brand awareness and positive sentiment may be especially important before important events like a startup's Series B.
</p>
<p>We propose to test the converse: Before financing rounds, might there be an abnormal amount of PR activity? Can we predict details on events like financing rounds based on mentions of a company on social media platforms like Twitter? Or is social media a noisy, meaningless indicator?</p>
<p>This project aims to use data analysis and predictive analytics to find correlations between tweets and startup funding rounds in order to shed light on potential importance of tweets and social media in general as indicators of startup success. Through a variety of models, we show that there is unfortunately minimal relationship between tweets and startup funding.</p>

<h2>
<a id="creating-pages-manually" class="anchor" href="#creating-pages-manually" aria-hidden="true"><span class="octicon octicon-link"></span></a>Data Collection</h2>
          
<h3>Startup Fundings</h3>

<!--
<p>We obtained startup funding round data (Series A, B, C, D, E, etc. with amounts raised and valuations) from <a href="https://github.com/kyang01/startup-analysis/blob/master/angel-scraping.ipynb">AngelList</a> and <a href="https://github.com/kyang01/startup-analysis/blob/master/cb-scraping.ipynb">Crunchbase</a> by scraping their search pages.</p>
 -->         
<p>We've downloaded our list of startups and their funding round info from <a href="https://angel.co/">AngelList</a>, a US website with extensive startup financial data, and <a href="https://www.crunchbase.com/">Crunchbase</a>, a database of the startup ecosystem. AngelList did not have an API, so we used the urllib2 Requests library to download each search page, then used BeautifulSoup to parse the page. Crunchbase also has no API, so we used the urllib2 Requests library in conjunction with Selenium Webdriver and BeautifulSoup. The data is located in the <a href="https://github.com/kyang01/startup-analysis/tree/master/data">data</a> folder.</p>

<h3>Tweets</h3>
     
<!--          
<p>We obtained tweet data (text, time, location, likes/retweets, etc.) by <a href="https://github.com/kyang01/startup-analysis/blob/master/twitter-scraping.ipynb">scraping</a> the Twitter search page. This necessitated significant <a href="https://github.com/kyang01/startup-analysis/blob/master/twitter-extraction.ipynb">cleaning</a> to be usable. Although Twitter has an API, it provides insufficient and outdated information.</p>
 -->  

<p> We downloaded our tweets by directly scraping <a href="https://twitter.com">Twitter</a> for mentions of startups on our list. While Twitter does have an API, its Search API is limited to an index of 6-9 days of tweets and its Timeline API is limited to up to 3200 tweets per timeline, not to mention rate limits on scraping both. We did initially write a <a href="https://github.com/kyang01/startup-analysis/blob/d98e6455038abec2d97097eb3009fd04c508799d/Mining-the-Social-Web-2nd-Edition/ipynb/Chapter%201%20-%20Mining%20Twitter.ipynb">code that used the API</a>, but decided instead to scrape via the Twitter <a href="https://twitter.com/search?q=">search page</a> with Selenium Webdriver browser scripts and BeautifulSoup. The tweets are located in the <a href="https://github.com/kyang01/startup-analysis/tree/master/data">data</a> folder.</p>


<h3>Translating Tweets</h3>

<p>The majority of our tweets were non-English, necessiting us to translate them in order to get an accurate model that works globally, not just for US startups. After filtering English tweets with the <a href="https://bitbucket.org/spirit/guess_language">guess_language</a> python library, we used the <a href="https://pypi.python.org/pypi/microsofttranslator/0.7">Microsoft Translator API</a> to translate all our tweets.</p>

<p>The majority of our tweets were non-English, so we chose to translate them via Microsoft Translator in order to have all-English results that would work globally. We took the nouns and adjectives from tweets, reducing sentences into words. We then converted words into their basic form, for example: "walk", "walking", "walks", "walked" =&gt;"walk". Each tweet is 140 characters at most, so we don't need to distinguish between sentences within tweets. </p>

<h2>
<a id="analysis" class="anchor" href="#" aria-hidden="true"><span class="octicon octicon-link"></span></a>Exploratory Data Analysis</h2>


<h3>
<a id="authors-and-contributors" class="anchor" href="#authors-and-contributors" aria-hidden="true"><span class="octicon octicon-link"></span></a>Startup Financial Analysis</h3>
<p>After preliminary analysis, we determined the average Series A funding round is approximately $6 million, the average Series B funding round is $11 million, the average Series C funding round is $16 million, and the average Series D funding round is around $17 million dollars. However, there is significant variance in the amount of money each company raises. Graphing it on a scale from 0 to $100 million, we see all Series are very much skewed right.</p>
<img src="images/series_funding.png">
<p>However, once we plot the log of the data instead, we are able to get a distribution that looks much less skewed and that we can treat as similar to normal.</p>
<img src="images/series_funding_log.png">


<h2>
    <a id="predictions" class="anchor" href="#" aria-hidden="true"><span class="octicon octicon-link"></span></a>Feature Extraction</h2>

<h3>Features from Tweet Metadata</h3>
<p>
  We get some metadata from scraping: the number of likes, the number of retweets, the date tweeted for each tweet. We group these by (company, funding_round) combination and compute the mean and standard deviation for each pair. We create features from these dates by computing the range of dates spanned among our ~200 tweets scraped, and the interquartile range. The intuition is that even though we can't scrape all the tweets made, if the range of dates are wide, given the fixed amount of tweets, then the tweets are made relatively infrequently. This might have some predictive power.
</p>



<h3>Features from Tweet Texts</h3>
<p>
Apart from metadata, we can extract a number of features from the text in the tweet itself. We compute text length, the number of hashtags, the number of persontags (tags of other Twitter accounts, beginning with @), the number of links, the proportion of tweets made by the company itself versus other people, the number of times tweets are directed toward the company.
</p>


<h3>Features from Funding Series and Market Sectors</h3> 
<p>
We create an indicator variable for each of the 4 funding rounds (A,B,C,D). When we look at the market sector data (that we get from scraping) we see that some sectors have a lot of companies in it (such as Biotech, has around 300) while most categories have very few (mostly less than 10). These categories that have few companies are not very useful because they are too dispersed and specific, but we think the top sectors that have a lot of companies are more useful, so we create indicator variables for top 10 sectors.
</p>

<h2>
    <a id="predictions" class="anchor" href="#" aria-hidden="true"><span class="octicon octicon-link"></span></a>Data Exploration and Feature Selection</h2>
 
<h3>Natural Language Processing</h3>

We parsed the text in each tweet using the pattern python library to extract nouns and adjectives. We removed punctuation and stopwords (from sklearn). We decided not to assign topics using LDA due to the heterogeneity of our tweets. We parsed the text into sentences and then tokenized the sentences into words. We then lemmatized the words, which means that we convert words into their basic form, for example: "walk", "walking", "walks", "walked" => "walk". Because each tweet is short (maximum 140 characters) we did not distinguish between sentences within tweets. 

<h3>Sentiment Analysis</h3>
<!--
<p>
  We used a sentiment dictionary grade the positivity, negativity, and objectivity of our words. “Positive" or “negative" words have positivity score&gt;0.5 or negativity score&gt;0.5, respectively. For each tweet, we took the average positivity and negativity score over all tokens, then summed up the total "positive" words and total "negative" words (usually 0,1, and rarely 2) in the tweet. To summarize, we have four features from sentiment analysis: average positivity, average negativity, positive count, negative count. For each company, funding round pair, we then take the average of these features for all their tweets.
</p>
-->

<p>
  We used the sentiment dictionary SentiWordNet 3.0, which assigns to words (both nouns and adjectives) three sentiment scores: positivity, negativity, objectivity. For each tweet, we took the average positivity score over all tokens and the average negativity score over all tokens. We also defined a word as "positive" or "negative" if it had positivity score>0.5 or negativity score>0.5 respectively. For each tweet, we then summed up the total "positive" words and total "negative" words (usually 0,1, and rarely 2). To summarize, we have four features from sentiment analysis: average positivity, average negativity, positive count, negative count. For each company, funding round pair, we then take the average of these features for all their tweets.
</p>

          
<h3>Correlation Analysis</h3>
<p>In this analysis, we only considered the 1200 startups that had more than 150 tweets and valid dates. With and without scaling both parameters, our results were mostly inconclusive. While some scaled v. scaled graphs displayed homoscedastic behavior (equal variance across features), their slopes tended to be very close to 0 and not at all linear.</p>
<br/>
<p>We plotted the unscaled 'Amount Raised' against all unscaled numerical features, and found no linear correlations.</p>
<img src="images/amount_vs_features.png">
<p>We then plotted the unscaled 'Amount Raised' against all numerical features normalized using boxcox transformation, and again found no linear correlations.</p>
<img src="images/amount_vs_features_scaled.png">
<p>We did not find linear correlations for scaled 'Amount Raised' against unscaled features either.</p>
<img src="images/scaled_amount_vs_features.png">
<p>We did find some homoskedastic correlations for scaled 'Amount Raised' against scaled features', but unfortunately these slopes were flat.</p>
<img src="images/scaled_amount_vs_features_scaled.png">
<p>From our correlation analysis, we infer that using SVR may be a better method for predictive modeling than linear regression.</p>
    
<h3>Principal Component Analysis</h3>
          
<p>Principal Component Analysis (PCA) tries to isolate a handful of linear combinations of features that "explain" most variances in the data. This is a descriptive, not predictive, technique, and it operates on the whole dataset without the training/testing division. Moreover, PCA is more informative if all features are suitably normalized, so no single feature can dominate the total variance. We then use Box Cox transformation on each column (the library chooses an appropriate parameter, different for each column, to make the resulting transformation approximately Normal.) The exception is the funding raised, which we use the log transformation (which is also a special case of Box Cox). This is justified because our plot shows that log(funding) looks Normal, and when we predict log(funding), reversing the function to get funding is more expedient. Our PCA shows that only a few (aggregated) features explain most of the variance. 3 top features explain 95% of the variance, while 5 top features explain 98% of the variance.</p>
                  <img src="images/pca_explain_var.png">
          
 <h2>Predictive Modeling </h2>
          
          <h3>Baseline Predictions</h3>
<p>Based off just the Series data, we're able to predict funding round amounts naively by predicting the average Series amount. We find the root mean squared error to be approximately $10 million, a baseline prediction we'll use to compare to our later, more complex predictions.</p>
          
<h3> Linear Regression</h3>
          
<h3> Support Vector Regression (SVR) </h3>
<p> 
  We suspect that the problem is not linear, so we turn to Support Vector Regression (SVR).  We split the data into the training data and the testing data, standardize the numerical features of each of the two datasets separately. We try three choices of kernels - rbf, linear and polynomial. For each choice of kernal, we use GridSearchCV with 5-fold cross validation to find the optimal parameters of the predictor over a reasonable (pre-determined) range of parameters. We then fit the predictor to the training data, predict it on the test data, and evaluate it by computing RMSE on log(funding). We found that rbf predictor with C=100 and gamma=0.01 is the best, with RMSE around 1. This result is comparable to linear regression.
</p>

          
          
     

<h3>Filler text</h3>
<p>You can <a href="https://github.com/blog/821" class="user-mention">@mention</a> a GitHub username to generate a link to their profile. The resulting <code>&lt;a&gt;</code> element will link to the contributor’s GitHub Profile. For example: In 2007, Chris Wanstrath (<a href="https://github.com/defunkt" class="user-mention">@defunkt</a>), PJ Hyett (<a href="https://github.com/pjhyett" class="user-mention">@pjhyett</a>), and Tom Preston-Werner (<a href="https://github.com/mojombo" class="user-mention">@mojombo</a>) founded GitHub. But then the Touhous came. Why is guessing regression values so OP -Roger -Kevin</p>

<h3>
<a id="support-or-contact" class="anchor" href="#support-or-contact" aria-hidden="true"><span class="octicon octicon-link"></span></a>Support or Contact</h3>

<p>Having trouble understanding Startup-Analysis? Check out <a href="https://github.com/contact">the code and implementations</a> directly.</p>
      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
      <p class="copyright">This project is maintained by <a href="https://github.com/kyang01">kyang01</a> with support from <a href="https://github.com/rogergzou">rogergzou</a> and <a href="https://github.com/jerryinfinity">jerryinfinity</a> and Melody</p>
        <p>Published with <a href="https://pages.github.com">GitHub Pages</a> for Harvard's CS109 Data Science Final Project</p>
      </footer>
    </div>

    

  </body>
</html>
Status API Training Shop Blog About Pricing
© 2015 GitHub, Inc. Terms Privacy Security Contact Help
